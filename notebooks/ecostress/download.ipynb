{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721e135-1f5e-4db1-91b1-f1fca1704860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import getpass\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import urllib\n",
    "import warnings\n",
    "from argparse import Namespace\n",
    "from http.cookiejar import CookieJar\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import set_start_method\n",
    "from typing import Tuple, List, Dict\n",
    "from datetime import datetime, timedelta, time\n",
    "import calendar\n",
    "import subprocess as sp\n",
    "import h5py\n",
    "from apis.ecostress_conv.ECOSTRESS_swath2grid import main\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "import bs4\n",
    "import certifi\n",
    "import requests\n",
    "import urllib3.util\n",
    "from osgeo import gdal, gdal_array, gdalconst, osr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "\n",
    "import pyproj\n",
    "from pyresample import geometry as geom\n",
    "from pyresample import kd_tree as kdt\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12a9e8-5d3b-43e3-8edd-596dd05aa37a",
   "metadata": {},
   "source": [
    "### The following is a slightly modified version of the ECOSTRESS swath2grid function found here https://git.earthdata.nasa.gov/projects/LPDUR/repos/ecostress_swath2grid/browse . This function takes an ECOSTRESS h5 data file and its corresponding L1B GEO file and converts the h5 file to a georeferenced tif file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49650832-202f-45f7-8c9a-2bd8d815460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecostress_swath_to_grid(args):\n",
    "\n",
    "    # --------------------------------SET ARGUMENTS TO VARIABLES------------------------------------- #\n",
    "    # Format and set input/working directory from user-defined arg\n",
    "    if args.dir[-1] != '/' and args.dir[-1] != '\\\\':\n",
    "        inDir = args.dir.strip(\"'\").strip('\"') + os.sep\n",
    "    else:\n",
    "        inDir = args.dir\n",
    "\n",
    "    # Find input directory\n",
    "    try:\n",
    "        os.chdir(inDir)\n",
    "    except FileNotFoundError:\n",
    "        print('error: input directory (--dir) provided does not exist or was not found')\n",
    "        sys.exit(2)\n",
    "\n",
    "    crsIN = args.proj  # Options include 'UTM' or 'GEO'\n",
    "\n",
    "    # -------------------------------------SET UP WORKSPACE------------------------------------------ #\n",
    "    # Create and set output directory\n",
    "    outDir = args.out_dir\n",
    "    if not os.path.exists(outDir):\n",
    "        os.makedirs(outDir)\n",
    "\n",
    "    # Create lists of ECOSTRESS HDF-EOS5 files (geo, data) in the directory\n",
    "    geoList = [f for f in os.listdir() if f.startswith('ECO') and f.endswith('.h5') and 'GEO' in f]\n",
    "    ecoList = [f for f in os.listdir() if f.startswith('ECO') and f.endswith('.h5') and 'GEO' not in f]\n",
    "\n",
    "    # Check to verify if any ECOSTRESS files are in the directory provided\n",
    "    if len(ecoList) == 0:\n",
    "        print(f'No ECOSTRESS files found in {inDir}')\n",
    "        sys.exit(2)\n",
    "    # -------------------------------------DEFINE FUNCTIONS------------------------------------------ #\n",
    "    # Write function to determine which UTM zone to use:\n",
    "    def utmLookup(lat, lon):\n",
    "        utm = str((math.floor((lon + 180) / 6) % 60) + 1)\n",
    "        if len(utm) == 1:\n",
    "            utm = '0' + utm\n",
    "        if lat >= 0:\n",
    "            epsg_code = '326' + utm\n",
    "        else:\n",
    "            epsg_code = '327' + utm\n",
    "        return epsg_code\n",
    "\n",
    "    for i, e in enumerate(ecoList):\n",
    "        i += 1\n",
    "        print('Processing: {} ({} of {})'.format(e, str(i), str(len(ecoList))))\n",
    "        f = h5py.File(e, \"r\")             # Read in ECOSTRESS HDF5-EOS data file\n",
    "        ecoName = e.split('.h5')[0]       # Keep original filename\n",
    "        eco_objs = []\n",
    "        f.visit(eco_objs.append)          # Retrieve list of datasets\n",
    "\n",
    "        # Search for relevant SDS inside data file\n",
    "        ecoSDS = [str(o) for o in eco_objs if isinstance(f[o], h5py.Dataset)]\n",
    "\n",
    "        # Added functionality for dataset subsetting (--sds argument)\n",
    "        if args.sds is not None:\n",
    "            sds = args.sds.split(',')\n",
    "            ecoSDS = [e for e in ecoSDS if e.endswith(tuple(sds))]\n",
    "            if ecoSDS == []:\n",
    "                print('No matching SDS layers found for {}'.format(e))\n",
    "                continue\n",
    "    # ---------------------------------CONVERT SWATH DATA TO GRID------------------------------------ #\n",
    "        # ALEXI products already gridded, bypass below\n",
    "        if 'ALEXI_USDA' in e:\n",
    "            cols, rows, dims = 3000, 3000, (3000, 3000)\n",
    "            ecoSDS = [s for s in ecoSDS if f[s].shape == dims]  # Omit NA layers/objs\n",
    "            if ecoSDS == []:\n",
    "                print('No matching SDS layers found for {}'.format(e))\n",
    "                continue\n",
    "        else:\n",
    "    # ---------------------------------IMPORT GEOLOCATION FILE--------------------------------------- #\n",
    "            geo = [g for g in geoList if e[-37:-10] in g]  # Match GEO filename--updated to exclude build ID\n",
    "            if len(geo) != 0 or 'L1B_MAP' in e:         # Proceed if GEO/MAP file\n",
    "                if 'L1B_MAP' in e:\n",
    "                    g = f                               # Map file contains lat/lon\n",
    "                else:\n",
    "                    g = h5py.File(geo[0], \"r\")               # Read in GEO file\n",
    "                geo_objs = []\n",
    "                g.visit(geo_objs.append)\n",
    "\n",
    "                # Search for relevant SDS inside data file\n",
    "                latSD = [str(o) for o in geo_objs if isinstance(g[o], h5py.Dataset) and '/latitude' in o]\n",
    "                lonSD = [str(o) for o in geo_objs if isinstance(g[o], h5py.Dataset) and '/longitude' in o]\n",
    "                lat = g[latSD[0]][()].astype(float)  # Open Lat array\n",
    "                lon = g[lonSD[0]][()].astype(float)  # Open Lon array\n",
    "                dims = lat.shape\n",
    "                ecoSDS = [s for s in ecoSDS if f[s].shape == dims]  # Omit NA layers/objs\n",
    "                if ecoSDS == []:\n",
    "                    print('No matching SDS layers found for {}'.format(e))\n",
    "                    continue\n",
    "    # --------------------------------SWATH TO GEOREFERENCED ARRAYS---------------------------------- #\n",
    "                swathDef = geom.SwathDefinition(lons=lon, lats=lat)\n",
    "                midLat, midLon = np.mean(lat), np.mean(lon)\n",
    "\n",
    "                if crsIN == 'UTM':\n",
    "                    if args.utmzone is None:\n",
    "                        epsg = utmLookup(midLat, midLon)  # Determine UTM zone that center of scene is in\n",
    "                    else:\n",
    "                        epsg = args.utmzone\n",
    "                    epsgConvert = pyproj.Proj(\"+init=EPSG:{}\".format(epsg))\n",
    "                    proj, pName = 'utm', 'Universal Transverse Mercator'\n",
    "                    projDict = {'proj': proj, 'zone': epsg[-2:], 'ellps': 'WGS84', 'datum': 'WGS84', 'units': 'm'}\n",
    "                    if epsg[2] == '7':\n",
    "                        projDict['south'] = 'True'  # Add for s. hemisphere UTM zones\n",
    "                    llLon, llLat = epsgConvert(np.min(lon), np.min(lat), inverse=False)\n",
    "                    urLon, urLat = epsgConvert(np.max(lon), np.max(lat), inverse=False)\n",
    "                    areaExtent = (llLon, llLat, urLon, urLat)\n",
    "                    ps = 70  # 70 is pixel size (meters)\n",
    "\n",
    "                if crsIN == 'GEO':\n",
    "                    # Use info from aeqd bbox to calculate output cols/rows/pixel size\n",
    "                    epsgConvert = pyproj.Proj(\"+proj=aeqd +lat_0={} +lon_0={}\".format(midLat, midLon))\n",
    "                    llLon, llLat = epsgConvert(np.min(lon), np.min(lat), inverse=False)\n",
    "                    urLon, urLat = epsgConvert(np.max(lon), np.max(lat), inverse=False)\n",
    "                    areaExtent = (llLon, llLat, urLon, urLat)\n",
    "                    cols = int(round((areaExtent[2] - areaExtent[0])/70))  # 70 m pixel size\n",
    "                    rows = int(round((areaExtent[3] - areaExtent[1])/70))\n",
    "                    '''Use no. rows and columns generated above from the aeqd projection\n",
    "                    to set a representative number of rows and columns, which will then be translated\n",
    "                    to degrees below, then take the smaller of the two pixel dims to determine output size'''\n",
    "                    epsg, proj, pName = '4326', 'longlat', 'Geographic'\n",
    "                    llLon, llLat, urLon, urLat = np.min(lon), np.min(lat), np.max(lon), np.max(lat)\n",
    "                    areaExtent = (llLon, llLat, urLon, urLat)\n",
    "                    projDict = pyproj.CRS(\"epsg:4326\")\n",
    "                    areaDef = geom.AreaDefinition(epsg, pName, proj, projDict, cols, rows, areaExtent)\n",
    "                    ps = np.min([areaDef.pixel_size_x, areaDef.pixel_size_y])  # Square pixels\n",
    "\n",
    "                cols = int(round((areaExtent[2] - areaExtent[0])/ps))  # Calculate the output cols\n",
    "                rows = int(round((areaExtent[3] - areaExtent[1])/ps))  # Calculate the output rows\n",
    "                areaDef = geom.AreaDefinition(epsg, pName, proj, projDict, cols, rows, areaExtent)\n",
    "                index, outdex, indexArr, distArr = kdt.get_neighbour_info(swathDef, areaDef, 210, neighbours=1)\n",
    "            else:\n",
    "                print('ECO1BGEO File not found for {}'.format(e))\n",
    "                continue\n",
    "\n",
    "    # ------------------LOOP THROUGH SDS CONVERT SWATH2GRID AND APPLY GEOREFERENCING----------------- #\n",
    "        print(ecoSDS)\n",
    "        for s in ecoSDS:\n",
    "            ecoSD = f[s][()]  # Create array and read dimensions\n",
    "\n",
    "            # Scale factor and add offset attribute names updated in build 6, accounted for below:\n",
    "            scaleName = [a for a in f[s].attrs if 'scale' in a.lower()] # '_Scale' or 'scale_factor'\n",
    "            addoffName = [a for a in f[s].attrs if 'offset' in a.lower()]\n",
    "\n",
    "            # Read SDS Attributes if available\n",
    "            try:\n",
    "                fv = int(f[s].attrs['_FillValue'])\n",
    "            except KeyError:\n",
    "                fv = None\n",
    "            except ValueError:\n",
    "                if f[s].attrs['_FillValue'] == b'n/a':\n",
    "                    fv = None\n",
    "                elif type(f[s].attrs['_FillValue'][0]) == np.float32:\n",
    "                    fv = np.nan\n",
    "                else:\n",
    "                    fv = f[s].attrs['_FillValue'][0]\n",
    "            try:\n",
    "                sf = f[s].attrs[scaleName[0]][0]\n",
    "            except:\n",
    "                sf = 1\n",
    "            try:\n",
    "                add_off = f[s].attrs[addoffName[0]][0]\n",
    "            except:\n",
    "                add_off = 0\n",
    "\n",
    "            if 'ALEXI_USDA' in e:  # USDA Contains proj info in metadata\n",
    "                if 'ET' in e:\n",
    "                    metaName = 'L3_ET_ALEXI Metadata'\n",
    "                else:\n",
    "                    metaName = 'L4_ESI_ALEXI Metadata'\n",
    "                gt = f[f\"{metaName}/Geotransform\"][()]\n",
    "                proj = f['{}/OGC_Well_Known_Text'.format(metaName)][()].decode('UTF-8')\n",
    "                sdGEO = ecoSD\n",
    "            else:\n",
    "                try:\n",
    "                    # Perform kdtree resampling (swath 2 grid conversion)\n",
    "                    sdGEO = kdt.get_sample_from_neighbour_info('nn', areaDef.shape, ecoSD, index, outdex, indexArr, fill_value=fv)\n",
    "                    ps = np.min([areaDef.pixel_size_x, areaDef.pixel_size_y])\n",
    "                    gt = [areaDef.area_extent[0], ps, 0, areaDef.area_extent[3], 0, -ps]\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            # Apply Scale Factor and Add Offset\n",
    "            sdGEO = sdGEO * sf + add_off\n",
    "\n",
    "            # Set fill value\n",
    "            if fv is not None:\n",
    "                sdGEO[sdGEO == fv * sf + add_off] = fv\n",
    "\n",
    "    # -------------------------------------EXPORT GEOTIFFS------------------------------------------- #\n",
    "            # For USDA, export to UTM, then convert to GEO\n",
    "            if 'ALEXI_USDA' in e and crsIN == 'GEO':\n",
    "                tempName = '{}{}_{}_{}.tif'.format(outDir, ecoName, s.rsplit('/')[-1], 'TEMP')\n",
    "                outName = tempName\n",
    "            elif args.bt and 'RAD' in e and 'radiance' in s:\n",
    "                outName = '{}{}_{}_{}.tif'.format(outDir, ecoName, s.rsplit('/')[-1], crsIN).replace('radiance', 'brightnesstemperature')\n",
    "            else:\n",
    "                outName = '{}/{}_{}_{}.tif'.format(outDir, ecoName, s.rsplit('/')[-1], crsIN)\n",
    "\n",
    "            # Get driver, specify dimensions, define and set output geotransform\n",
    "            height, width = sdGEO.shape  # Define geotiff dimensions\n",
    "            driv = gdal.GetDriverByName('GTiff')\n",
    "            dataType = gdal_array.NumericTypeCodeToGDALTypeCode(sdGEO.dtype)\n",
    "            print(outName)\n",
    "            d = driv.Create(outName, width, height, 1, dataType)\n",
    "            d.SetGeoTransform(gt)\n",
    "\n",
    "            # Create and set output projection, write output array data\n",
    "            if 'ALEXI_USDA' in e:\n",
    "                d.SetProjection(proj)\n",
    "            else:\n",
    "                # Define target SRS\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromEPSG(int(epsg))\n",
    "                d.SetProjection(srs.ExportToWkt())\n",
    "            band = d.GetRasterBand(1)\n",
    "            band.WriteArray(sdGEO)\n",
    "\n",
    "            # Define fill value if it exists, if not, set to mask fill value\n",
    "            if fv is not None and fv != 'NaN':\n",
    "                band.SetNoDataValue(fv)\n",
    "            else:\n",
    "                try:\n",
    "                    band.SetNoDataValue(int(sdGEO.fill_value))\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                except TypeError:\n",
    "                    pass\n",
    "\n",
    "            band.FlushCache()\n",
    "            d, band = None, None\n",
    "\n",
    "            if 'ALEXI_USDA' in e and crsIN == 'GEO':\n",
    "                # Define target SRS\n",
    "                srs = osr.SpatialReference()\n",
    "                srs.ImportFromEPSG(int('4326'))\n",
    "                srs = srs.ExportToWkt()\n",
    "\n",
    "                # Open temp file, get default vals for target dims & geotransform\n",
    "                dd = gdal.Open(tempName, gdalconst.GA_ReadOnly)\n",
    "                vrt = gdal.AutoCreateWarpedVRT(dd, None, srs, gdal.GRA_NearestNeighbour, 0.125)\n",
    "\n",
    "                # Create the final warped raster\n",
    "                outName = '{}{}_{}_{}.tif'.format(outDir, ecoName, s.rsplit('/')[-1], crsIN)\n",
    "                d = driv.CreateCopy(outName, vrt)\n",
    "                dd, d, vrt = None, None, None\n",
    "                os.remove(tempName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392dcf4e-0298-47e1-97bb-dad36c57dca6",
   "metadata": {},
   "source": [
    "### The following class defines an API for downloading ECOSTRESS Water Use Efficiency (WUE) data for a specific time and location. The WUE data is downloaded alongside the ECOSTRESS Level 1B GEO files and L2 Cloud files for georeferencing and cloud correction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b244d-3db4-4056-b3f4-b1702c547733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAPI:\n",
    "    \"\"\"\n",
    "    Defines all the attributes and methods common to the child APIs.\n",
    "    \"\"\"\n",
    "    PROJ_DIR = os.path.dirname(os.path.dirname(__file__))\n",
    "    _BASE_WUE_URL = 'https://e4ftl01.cr.usgs.gov/ECOSTRESS/ECO4WUE.001/'\n",
    "    _BASE_GEO_URL = 'https://e4ftl01.cr.usgs.gov/ECOSTRESS/ECO1BGEO.001/'\n",
    "    _BASE_CLOUD_URL = 'https://e4ftl01.cr.usgs.gov/ECOSTRESS/ECO2CLD.001/'\n",
    "\n",
    "    _XML_DIR = os.path.join(PROJ_DIR, 'xml_files')\n",
    "\n",
    "    def __init__(self, username: str = None, password: str = None, lazy: bool = False):\n",
    "        \"\"\"`\n",
    "        Initializes the common attributes required for each data type's API\n",
    "        \"\"\"\n",
    "        self._username = os.environ.get('FIRE_RX_USER', username)\n",
    "        self._password = os.environ.get('FIRE_RX_PASS', password)\n",
    "        self._core_count = os.cpu_count()\n",
    "        if not lazy:\n",
    "            self._configure()\n",
    "        self._file_re = None\n",
    "        self._tif_re = None\n",
    "\n",
    "        os.makedirs(self._XML_DIR, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve_links(url: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Creates a list of all the links found on a webpage\n",
    "        Args:\n",
    "            url (str): The URL of the webpage for which you would like a list of links\n",
    "\n",
    "        Returns:\n",
    "            (list): All the links on the input URL's webpage\n",
    "        \"\"\"\n",
    "        request = requests.get(url)\n",
    "        soup = bs4.BeautifulSoup(request.text, 'html.parser')\n",
    "        return [link.get('href') for link in soup.find_all('a')]\n",
    "\n",
    "    @staticmethod\n",
    "    def _cred_query() -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Ask the user for their urs.earthdata.nasa.gov username and login\n",
    "        Returns:\n",
    "            username (str): urs.earthdata.nasa.gov username\n",
    "            password (str): urs.earthdata.nasa.gov password\n",
    "        \"\"\"\n",
    "        print('Please input your earthdata.nasa.gov username and password. If you do not have one, you can register'\n",
    "              ' here: https://urs.earthdata.nasa.gov/users/new')\n",
    "        username = input('Username:')\n",
    "        password = getpass.getpass('Password:', stream=None)\n",
    "\n",
    "        return username, password\n",
    "\n",
    "    def _configure(self) -> None:\n",
    "        \"\"\"\n",
    "        Queries the user for credentials and configures SSL certificates\n",
    "        \"\"\"\n",
    "        if self._username is None or self._password is None:\n",
    "            username, password = self._cred_query()\n",
    "\n",
    "            self._username = username\n",
    "            self._password = password\n",
    "\n",
    "        # This is a macOS thing... need to find path to SSL certificates and set the following environment variables\n",
    "        ssl_cert_path = certifi.where()\n",
    "        if 'SSL_CERT_FILE' not in os.environ or os.environ['SSL_CERT_FILE'] != ssl_cert_path:\n",
    "            os.environ['SSL_CERT_FILE'] = ssl_cert_path\n",
    "\n",
    "        if 'REQUESTS_CA_BUNDLE' not in os.environ or os.environ['REQUESTS_CA_BUNDLE'] != ssl_cert_path:\n",
    "            os.environ['REQUESTS_CA_BUNDLE'] = ssl_cert_path\n",
    "\n",
    "    def _download(self, query: Tuple[str, str], retry: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Downloads data from the NASA earthdata servers. Authentication is established using the username and password\n",
    "        found in the local ~/.netrc file.\n",
    "        Args:\n",
    "            query (tuple): Contains the remote location and the local path destination, respectively\n",
    "        \"\"\"\n",
    "        link = query[0]\n",
    "        dest = query[1]\n",
    "        if os.path.exists(dest):\n",
    "            print(f'Skipping {dest}')\n",
    "            return\n",
    "\n",
    "        pm = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
    "        pm.add_password(None, \"https://urs.earthdata.nasa.gov\", self._username, self._password)\n",
    "        cookie_jar = CookieJar()\n",
    "        opener = urllib.request.build_opener(\n",
    "            urllib.request.HTTPBasicAuthHandler(pm),\n",
    "            urllib.request.HTTPCookieProcessor(cookie_jar)\n",
    "        )\n",
    "        urllib.request.install_opener(opener)\n",
    "        myrequest = urllib.request.Request(link)\n",
    "        response = urllib.request.urlopen(myrequest)\n",
    "        response.begin()\n",
    "        with open(dest, 'wb') as fd:\n",
    "            while True:\n",
    "                chunk = response.read()\n",
    "                if chunk:\n",
    "                    fd.write(chunk)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        if not self._verify_hdf_file(dest):\n",
    "            os.remove(dest)\n",
    "            if retry < 1:\n",
    "                self._download(query, retry=1)\n",
    "\n",
    "    def download_time_series(self, queries: List[Tuple[str, str]], outdir: str):\n",
    "        \"\"\"\n",
    "        Attempts to create download requests for each query, if that fails then makes each request in series.\n",
    "        Args:\n",
    "            queries (list): List of tuples containing the remote and local locations for each request\n",
    "        Returns:\n",
    "            outdir (str): Path to the output file directory\n",
    "        \"\"\"\n",
    "        # From earthlab firedpy package\n",
    "        if len(queries) > 0:\n",
    "            print(\"Retrieving data... skipping over any cached files\")\n",
    "\n",
    "            with Pool(int(self._core_count / 2)) as pool:\n",
    "                for _ in tqdm(pool.imap_unordered(self._download, queries), total=len(queries)):\n",
    "                    pass\n",
    "\n",
    "        print(f'Wrote {len(queries)} files to {outdir}')\n",
    "\n",
    "    @staticmethod\n",
    "    def _verify_hdf_file(file_path: str) -> bool:\n",
    "        try:\n",
    "            h5py.File(file_path)\n",
    "            return True\n",
    "        except OSError:\n",
    "            return False\n",
    "\n",
    "    def _parse_bbox_from_xml(self, xml_url: str) -> Polygon:\n",
    "        filename = os.path.basename(xml_url)\n",
    "        file_path = os.path.join(self._XML_DIR, filename)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            # Send a GET request with HTTP Basic Authentication\n",
    "            pm = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
    "            pm.add_password(None, \"https://urs.earthdata.nasa.gov\", self._username, self._password)\n",
    "            cookie_jar = CookieJar()\n",
    "            opener = urllib.request.build_opener(\n",
    "                urllib.request.HTTPBasicAuthHandler(pm),\n",
    "                urllib.request.HTTPCookieProcessor(cookie_jar)\n",
    "            )\n",
    "            urllib.request.install_opener(opener)\n",
    "            myrequest = urllib.request.Request(xml_url)\n",
    "            with urllib.request.urlopen(myrequest) as response:\n",
    "                xml_content = response.read()\n",
    "\n",
    "                # Write XML content to file\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(xml_content)\n",
    "\n",
    "        # Parse the XML content\n",
    "        root = ET.fromstring(open(file_path, 'rb').read())\n",
    "        bounding_rect = root.find('.//BoundingRectangle')\n",
    "        west = float(bounding_rect.find('WestBoundingCoordinate').text)\n",
    "        north = float(bounding_rect.find('NorthBoundingCoordinate').text)\n",
    "        east = float(bounding_rect.find('EastBoundingCoordinate').text)\n",
    "        south = float(bounding_rect.find('SouthBoundingCoordinate').text)\n",
    "        return Polygon([(west, north), (east, north), (east, south), (west, south)])\n",
    "\n",
    "    def _overlaps_bbox(self, target_bbox: List[int], xml_url: str):\n",
    "        # Now apply spatial filter by downloading the xml files and checking if they overlap the bounding box\n",
    "        min_lon, min_lat, max_lon, max_lat = target_bbox[0], target_bbox[1], target_bbox[2], target_bbox[3]\n",
    "        target_bbox = Polygon([(min_lon, max_lat), (max_lon, max_lat), (max_lon, min_lat), (min_lon, min_lat)])\n",
    "        file_bbox = self._parse_bbox_from_xml(xml_url)\n",
    "        return file_bbox.intersects(target_bbox)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_raster(output_path: str, columns: int, rows: int, n_band: int = 1,\n",
    "                       gdal_data_type: int = gdal.GDT_Float32, driver: str = r'GTiff'):\n",
    "        \"\"\"\n",
    "        Credit:\n",
    "        https://gis.stackexchange.com/questions/290776/how-to-create-a-tiff-file-using-gdal-from-a-numpy-array-and-\n",
    "        specifying-nodata-va\n",
    "\n",
    "        Creates a blank raster for data to be written to\n",
    "        Args:\n",
    "            output_path (str): Path where the output tif file will be written to\n",
    "            columns (int): Number of columns in raster\n",
    "            rows (int): Number of rows in raster\n",
    "            n_band (int): Number of bands in raster\n",
    "            gdal_data_type (int): Data type for data written to raster\n",
    "            driver (str): Driver for conversion\n",
    "        \"\"\"\n",
    "        # create driver\n",
    "        driver = gdal.GetDriverByName(driver)\n",
    "\n",
    "        output_raster = driver.Create(output_path, columns, rows, n_band, eType=gdal_data_type)\n",
    "        return output_raster\n",
    "\n",
    "    @staticmethod\n",
    "    def _numpy_array_to_raster(output_path: str, numpy_array: np.array, geo_transform,\n",
    "                               projection, n_bands: int = 1, no_data: int = np.nan,\n",
    "                               gdal_data_type: int = gdal.GDT_Float32):\n",
    "        \"\"\"\n",
    "        Returns a gdal raster data source\n",
    "        Args:\n",
    "            output_path (str): Full path to the raster to be written to disk\n",
    "            numpy_array (np.array): Numpy array containing data to write to raster\n",
    "            geo_transform (gdal GeoTransform): tuple of six values that represent the top left corner coordinates, the\n",
    "            pixel size in x and y directions, and the rotation of the image\n",
    "            n_bands (int): The band to write to in the output raster\n",
    "            no_data (int): Value in numpy array that should be treated as no data\n",
    "            gdal_data_type (int): Gdal data type of raster (see gdal documentation for list of values)\n",
    "        \"\"\"\n",
    "        rows, columns = numpy_array.shape[0], numpy_array.shape[1]\n",
    "\n",
    "        # create output raster\n",
    "        output_raster = BaseAPI._create_raster(output_path, int(columns), int(rows), n_bands, gdal_data_type)\n",
    "\n",
    "        output_raster.SetProjection(projection)\n",
    "        output_raster.SetGeoTransform(geo_transform)\n",
    "        for i in range(n_bands):\n",
    "            output_band = output_raster.GetRasterBand(i + 1)\n",
    "            output_band.SetNoDataValue(no_data)\n",
    "            output_band.WriteArray(numpy_array[:, :, i] if numpy_array.ndim == 3 else numpy_array)\n",
    "            output_band.FlushCache()\n",
    "            output_band.ComputeStatistics(False)\n",
    "\n",
    "        if not os.path.exists(output_path):\n",
    "            raise Exception('Failed to create raster: %s' % output_path)\n",
    "\n",
    "        return output_path\n",
    "\n",
    "\n",
    "class L4WUE(BaseAPI):\n",
    "\n",
    "    def __init__(self, username: str = None, password: str = None, lazy: bool = False):\n",
    "        super().__init__(username=username, password=password, lazy=lazy)\n",
    "        common_regex = r'\\_(?P<orbit>\\d{5})\\_(?P<scene_id>\\d{3})\\_(?P<year>\\d{4})(?P<month>\\d{2})(?P<day>\\d{2})T(?P<hour>\\d{2})(?P<minute>\\d{2})(?P<second>\\d{2})\\_(?P<build_id>\\d{4})\\_(?P<version>\\d{2})\\.h5$'\n",
    "        self._wue_file_re = r'ECOSTRESS\\_L4\\_WUE' + common_regex\n",
    "        self._bgeo_file_re = r'ECOSTRESS\\_L1B\\_GEO' + common_regex\n",
    "        self._cloud_file_re = r'ECOSTRESS\\_L2\\_CLOUD' + common_regex\n",
    "        self._cloud_file_tif_re = r'ECOSTRESS\\_L2\\_CLOUD' + common_regex.replace('.h5', '_CloudMask_GEO.tif')\n",
    "        self._wue_tif_re = r'ECOSTRESS\\_L4\\_WUE' + common_regex.replace('.h5', '_WUEavg_GEO.tif')\n",
    "        self._db_re = r'(?P<year>\\d{4})-(?P<month>\\d{2})-(?P<day>\\d{2}) (?P<hour>\\d{2}):(?P<minute>\\d{2}):(?P<second>\\d{2})\\_(?P<min_lon>\\-?\\d+\\.\\d+)\\_(?P<max_lon>\\-?\\d+\\.\\d+)\\_(?P<min_lat>\\-?\\d+\\.\\d+)\\_(?P<max_lat>\\-?\\d+\\.\\d+)\\_(?P<lon_res>\\-?\\d+\\.\\d+)\\_(?P<lat_res>\\-?\\d+\\.\\d+)\\.db$'\n",
    "        self._res = 0.0006298419\n",
    "        self._projection = 'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_last_day_of_month(year, month):\n",
    "        # monthrange returns a tuple (weekday of first day of the month, number of days in month)\n",
    "        _, num_days = calendar.monthrange(year, month)\n",
    "        return num_days\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_file_key(file_group_dict: Dict[str, str]) -> tuple:\n",
    "        return (file_group_dict['orbit'], file_group_dict['scene_id'], file_group_dict['year'],\n",
    "                file_group_dict['month'], file_group_dict['day'], file_group_dict['hour'], file_group_dict['minute'],\n",
    "                file_group_dict['second'])\n",
    "\n",
    "    def _worker(self, args):\n",
    "        day_url, day_file, bbox, hour_start, hour_end = args\n",
    "        match = re.match(self._wue_file_re, day_file)\n",
    "        if match:\n",
    "            groups = match.groupdict()\n",
    "            if hour_start <= int(groups['hour']) < hour_end:\n",
    "                url = urllib.parse.urljoin(day_url, day_file)\n",
    "                if self._overlaps_bbox(bbox, url + '.xml'):\n",
    "                    return os.path.basename(day_url[:-1]), url\n",
    "        return None\n",
    "\n",
    "    def _find_matching_urls(self, base_url: str, date: str, links: List[str], file_re: str) -> Dict[tuple, str]:\n",
    "        file_dict = {}\n",
    "        for link in links:\n",
    "            match = re.match(file_re, link)\n",
    "            if match:\n",
    "                file_group_dict = match.groupdict()\n",
    "                key = self._generate_file_key(file_group_dict)\n",
    "                file_dict[key] = urllib.parse.urljoin(base_url + '/' + date + '/', link)\n",
    "\n",
    "        return file_dict\n",
    "\n",
    "    def gather_file_links(self, year: int, month_start: int, month_end: int, hour_start: int,\n",
    "                          hour_end: int, bbox: List[int]) -> List[Tuple[str, str, str]]:\n",
    "        start_date = datetime(year, month_start, 1)\n",
    "        end_date = datetime(year, month_end, self._get_last_day_of_month(year, month_end))\n",
    "\n",
    "        day_urls = []\n",
    "        while start_date <= end_date:\n",
    "            day_urls.append(urllib.parse.urljoin(self._BASE_WUE_URL, start_date.strftime('%Y.%m.%d') + '/'))\n",
    "            start_date += timedelta(days=1)\n",
    "\n",
    "        # Prepare arguments for multiprocessing\n",
    "        args = []\n",
    "        for day_url in day_urls:\n",
    "            day_files = self.retrieve_links(day_url)\n",
    "            for day_file in day_files:\n",
    "                args.append((day_url, day_file, bbox, hour_start, hour_end))\n",
    "\n",
    "        geo_lookup = collections.defaultdict(list)\n",
    "\n",
    "        with Pool(mp.cpu_count() - 1) as pool:\n",
    "            # Using tqdm to create a progress bar\n",
    "            for result in tqdm(pool.imap_unordered(self._worker, args), total=len(args),\n",
    "                               desc='Finding overlapping files'):\n",
    "                if result:\n",
    "                    day, url = result\n",
    "                    geo_lookup[day].append(url)\n",
    "\n",
    "        # Now find the GEO urls. The versions are not always the same (this doesn't matter for the swath2grid function)\n",
    "        # so you cannot infer the GEO url from the WUE url. What should match is the orbit, scene_id, and date.\n",
    "        matched_urls = []\n",
    "        for date, wue_file_links in geo_lookup.items():\n",
    "            geo_date_url = urllib.parse.urljoin(self._BASE_GEO_URL, date)\n",
    "            cloud_date_url = urllib.parse.urljoin(self._BASE_CLOUD_URL, date)\n",
    "            geo_links = self.retrieve_links(geo_date_url)\n",
    "            cloud_links = self.retrieve_links(cloud_date_url)\n",
    "\n",
    "            geo_file_dict = self._find_matching_urls(self._BASE_GEO_URL, date, geo_links, self._bgeo_file_re)\n",
    "            cloud_file_dict = self._find_matching_urls(self._BASE_CLOUD_URL, date, cloud_links, self._cloud_file_re)\n",
    "\n",
    "            for wue_file_link in wue_file_links:\n",
    "                group_dict = re.match(self._wue_file_re, os.path.basename(wue_file_link)).groupdict()\n",
    "                key = self._generate_file_key(group_dict)\n",
    "                if key in geo_file_dict and key in cloud_file_dict:\n",
    "                    matched_urls.append((wue_file_link, geo_file_dict[key], cloud_file_dict[key]))\n",
    "\n",
    "        return matched_urls\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_cloud_mask(cloud_data: np.array) -> np.array:\n",
    "        mask = np.zeros_like(cloud_data)\n",
    "        for row_idx, col_idx in np.ndindex(cloud_data.shape):\n",
    "            v = cloud_data[row_idx, col_idx]\n",
    "            bits = [bool(int(c)) for c in bin(v)[2:].zfill(8)]\n",
    "            bits.reverse()\n",
    "            mask[row_idx, col_idx] = 1 if (\n",
    "                    bits[0] and\n",
    "                    (bits[1] or bits[2]) and\n",
    "                    not bits[6] and\n",
    "                    not bits[7]\n",
    "            ) else 0\n",
    "\n",
    "        print(any(mask.flatten()), np.count_nonzero(mask.flatten()), np.count_nonzero(mask.flatten()) / mask.size)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def process_region(self, args):\n",
    "        overlapping_files, matching_cloud_files, mosaic_array, region_bounds, outfile = args\n",
    "        region_min_lon, region_max_lon, region_min_lat, region_max_lat = region_bounds\n",
    "\n",
    "        index_to_median = collections.defaultdict(list)\n",
    "        for f_i, file in enumerate(overlapping_files):\n",
    "            g = gdal.Open(file)\n",
    "            gt = g.GetGeoTransform()\n",
    "            data = g.ReadAsArray()\n",
    "\n",
    "            cloud_file = gdal.Open(matching_cloud_files[file])\n",
    "            cloud_mask = self._create_cloud_mask(cloud_file.ReadAsArray())\n",
    "\n",
    "            t1 = time.time()\n",
    "            for i, row in enumerate(data):\n",
    "                row_lat = gt[3] + (gt[5] * i)\n",
    "                if not region_min_lat <= row_lat <= region_max_lat:\n",
    "                    continue\n",
    "                for j, column in enumerate(row):\n",
    "                    row_lon = gt[0] + (gt[1] * j)\n",
    "                    region_indices = (int((region_max_lat - row_lat) / self._res),\n",
    "                                      int((row_lon - region_min_lon) / self._res))\n",
    "                    val = data[i, j]\n",
    "                    index_to_median[region_indices].append(val if val >= 0 and not cloud_mask[i, j] else np.nan)\n",
    "                if i % 1000 == 0:\n",
    "                    print(f'{i} / {data.shape[0]} File {f_i + 1} / {len(overlapping_files)} {time.time() - t1}')\n",
    "\n",
    "        for k, v in index_to_median.items():\n",
    "            mosaic_array[k] = np.nanmedian(v)\n",
    "\n",
    "        mosaic_array = mosaic_array.astype(np.float32)\n",
    "        self._numpy_array_to_raster(\n",
    "            outfile, mosaic_array, [region_min_lon, self._res, 0, region_max_lat, 0, -self._res],\n",
    "            self._projection)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mosaic(in_dir: str, out_file: str):\n",
    "        # List all TIFF files in the directory\n",
    "        all_files = glob(os.path.join(in_dir, \"*.tif\"))\n",
    "\n",
    "        # List to hold opened raster datasets\n",
    "        src_files_to_mosaic = []\n",
    "\n",
    "        # Open and append each raster to the list\n",
    "        for fp in all_files:\n",
    "            src = rasterio.open(fp)\n",
    "            src_files_to_mosaic.append(src)\n",
    "\n",
    "        # Merge function returns a single mosaic array and the transformation info\n",
    "        mosaic, out_trans = merge(src_files_to_mosaic)\n",
    "\n",
    "        # Copy the metadata\n",
    "        out_meta = src.meta.copy()\n",
    "\n",
    "        # Update the metadata to reflect the number of layers in the mosaic\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": mosaic.shape[1],\n",
    "                         \"width\": mosaic.shape[2],\n",
    "                         \"transform\": out_trans\n",
    "                         })\n",
    "\n",
    "        # Write the mosaic raster to disk\n",
    "        with rasterio.open(out_file, \"w\", **out_meta) as dest:\n",
    "            dest.write(mosaic)\n",
    "\n",
    "    def _create_composite(self, file_dir: str, year: int, month_start: int, month_end: int, hour_start: int,\n",
    "                          hour_end: int, bbox: List[int], out_dir: str, n_regions: int = 10, processes: int = 6):\n",
    "\n",
    "        # First get all the files, filtering on the hour month and bounding box\n",
    "        min_lon, min_lat, max_lon, max_lat = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "\n",
    "        cloud_file_dict = {}\n",
    "        for file in os.listdir(file_dir):\n",
    "            match = re.match(self._cloud_file_tif_re, file)\n",
    "            if match:\n",
    "                key = self._generate_file_key(match.groupdict())\n",
    "                cloud_file_dict[key] = os.path.join(file_dir, file)\n",
    "\n",
    "        print(cloud_file_dict)\n",
    "\n",
    "        file_regions = {}\n",
    "        for file in os.listdir(file_dir):\n",
    "            if re.match(self._wue_tif_re, file):\n",
    "                file_path = os.path.join(file_dir, file)\n",
    "                g = gdal.Open(file_path)\n",
    "                gt = g.GetGeoTransform()\n",
    "                dim = g.ReadAsArray().shape\n",
    "                bounds = gt[0], gt[0] + (gt[1] * dim[1]), gt[3] + (gt[5] * dim[0]), gt[3]\n",
    "                file_regions[file_path] = bounds\n",
    "\n",
    "        matching_files = {}\n",
    "        matching_cloud_files = {}\n",
    "        for file, bounds in file_regions.items():\n",
    "            group_dict = re.match(self._wue_tif_re, os.path.basename(file)).groupdict()\n",
    "            key = self._generate_file_key(group_dict)\n",
    "            if (\n",
    "                    hour_start <= int(group_dict['hour']) < hour_end and\n",
    "                    month_start <= int(group_dict['month']) <= month_end and\n",
    "                    int(group_dict['year']) == year and\n",
    "                    bounds[0] < max_lon and\n",
    "                    bounds[1] > min_lon and\n",
    "                    bounds[2] < max_lat and\n",
    "                    bounds[3] > min_lat\n",
    "            ) and key in cloud_file_dict:\n",
    "                matching_files[file] = bounds\n",
    "                matching_cloud_files[file] = cloud_file_dict[key]\n",
    "\n",
    "        # Create an empty array with 70m x 70m resolution\n",
    "        min_lon = min([c[0] for c in matching_files.values()])\n",
    "        max_lon = max([c[1] for c in matching_files.values()])\n",
    "        min_lat = min([c[2] for c in matching_files.values()])\n",
    "        max_lat = max([c[3] for c in matching_files.values()])\n",
    "\n",
    "        n_rows = int((max_lat - min_lat) / self._res)\n",
    "        n_cols = int((max_lon - min_lon) / self._res)\n",
    "\n",
    "        # Define the number of regions\n",
    "        num_regions = n_regions\n",
    "\n",
    "        # Define the size of each region\n",
    "        region_height = n_rows // num_regions\n",
    "\n",
    "        # Prepare arguments for each region\n",
    "        args = []\n",
    "        for i in range(0, n_rows, region_height):\n",
    "            h = min(region_height, n_rows - i)\n",
    "            r_min_lat = min_lat + (i * self._res)\n",
    "            r_max_lat = min(max_lat, r_min_lat + (h * self._res))\n",
    "            r_outfile = os.path.join(out_dir, f'ECOSTRESS_L4_WUE_{min_lon}_{max_lon}_{r_min_lat}_{r_max_lat}_{i}.tif')\n",
    "\n",
    "            if os.path.exists(r_outfile):\n",
    "                continue\n",
    "\n",
    "            mosaic_array = np.full((h, n_cols), np.nan)\n",
    "\n",
    "            overlapping_files = []\n",
    "            for file, bounds in matching_files.items():\n",
    "                if bounds[0] < max_lon and bounds[1] > min_lon and bounds[2] < r_max_lat and bounds[3] > r_min_lat:\n",
    "                    overlapping_files.append(file)\n",
    "            args.append((overlapping_files, matching_cloud_files, mosaic_array, (min_lon, max_lon, r_min_lat, r_max_lat), r_outfile))\n",
    "\n",
    "        t1 = time.time()\n",
    "        with mp.Pool(processes=processes) as pool:\n",
    "            results = pool.map(self.process_region, args)\n",
    "        print(time.time() - t1, 'total time')\n",
    "\n",
    "    @staticmethod\n",
    "    def _tif_file_exists(dest: str) -> bool:\n",
    "        return (os.path.exists(os.path.join(os.path.dirname(os.path.dirname(dest)), 'geo_tiffs',\n",
    "                                            os.path.basename(dest).strip('.h5') + '_WUEavg_GEO.tif')) or\n",
    "                glob(os.path.join(os.path.dirname(os.path.dirname(dest)), 'geo_tiffs',\n",
    "                                  os.path.basename(dest).strip('.h5').replace('L1B_GEO', 'L4_WUE')[:43] +\n",
    "                                  '*' + '_WUEavg_GEO.tif')))\n",
    "\n",
    "    def download_composite(self, year: int, month_start: int, month_end: int, hour_start: int, hour_end: int,\n",
    "                           bbox: List[int], batch_size: int = 50):\n",
    "        set_start_method('fork')\n",
    "\n",
    "        out_dir = os.path.join(self.PROJ_DIR, 'apis', f'{year}_{month_start}_{month_end}_{hour_start}_{hour_end}')\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        batch_out_dir = os.path.join(out_dir, 'batch')\n",
    "        os.makedirs(batch_out_dir, exist_ok=True)\n",
    "\n",
    "        geo_tiff_dir = os.path.join(out_dir, 'geo_tiffs')\n",
    "        os.makedirs(geo_tiff_dir, exist_ok=True)\n",
    "\n",
    "        # Download the files if they don't exist\n",
    "        urls = self.gather_file_links(year, month_start, month_end, hour_start, hour_end, bbox)\n",
    "\n",
    "        # The geo files are large enough that it makes sense to delete them periodically by processing the swaths in\n",
    "        # batches\n",
    "        url_batches = [urls[i:i + batch_size] for i in range(0, len(urls), batch_size)]\n",
    "\n",
    "        for url_batch in url_batches:\n",
    "            os.makedirs(batch_out_dir, exist_ok=True)\n",
    "\n",
    "            wue_requests = [(url_pair[0], os.path.join(batch_out_dir, os.path.basename(url_pair[0]))) for\n",
    "                            url_pair in url_batch if\n",
    "                            not self._tif_file_exists(os.path.join(batch_out_dir, os.path.basename(url_pair[0])))]\n",
    "\n",
    "            geo_requests = [(url_pair[1], os.path.join(batch_out_dir, os.path.basename(url_pair[1]))) for\n",
    "                            url_pair in url_batch if\n",
    "                            not self._tif_file_exists(os.path.join(batch_out_dir, os.path.basename(url_pair[1])))]\n",
    "\n",
    "            cloud_requests = [(url_pair[2], os.path.join(batch_out_dir, os.path.basename(url_pair[2]))) for\n",
    "                              url_pair in url_batch if\n",
    "                              not self._tif_file_exists(os.path.join(batch_out_dir, os.path.basename(url_pair[2])))]\n",
    "\n",
    "            if wue_requests:\n",
    "                print(f'Downloading WUE requests {wue_requests}')\n",
    "                self.download_time_series(wue_requests, batch_out_dir)\n",
    "\n",
    "            if geo_requests:\n",
    "                print(f'Downloading GEO requests {geo_requests}')\n",
    "                print(geo_requests)\n",
    "                self.download_time_series(geo_requests, batch_out_dir)\n",
    "\n",
    "            if cloud_requests:\n",
    "                print(f'Downloading CLOUD requests {cloud_requests}')\n",
    "                self.download_time_series(cloud_requests, batch_out_dir)\n",
    "\n",
    "            # Convert them into TIFs\n",
    "            if geo_requests or wue_requests or cloud_requests:\n",
    "                ecostress_swath_to_grid(Namespace(proj='GEO', dir=batch_out_dir, out_dir=geo_tiff_dir, sds=None, utmzone=None, bt=None))\n",
    "                shutil.rmtree(batch_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81dfdd2-3608-4486-a5bd-1c4d5f1e540a",
   "metadata": {},
   "source": [
    "### The following shows an example of how to use the above class to download ECOSTRESS WUE data for a desired time and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf462db-6785-433f-8aae-c1fc228e7cc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L4QUE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wue_api \u001b[38;5;241m=\u001b[39m \u001b[43mL4QUE\u001b[49m(username\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_earthdata_username\u001b[39m\u001b[38;5;124m\"\u001b[39m, password \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_earthdata_password\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L4QUE' is not defined"
     ]
    }
   ],
   "source": [
    "wue_api = L4QUE(username=\"my_earthdata_username\", password = \"my_earthdata_password\")\n",
    "\n",
    "year = 2022  # Year for which to download the WUE data\n",
    "month_start = 5  # Temporal range will start in May\n",
    "month_end = 8  # Temporal range will end in August\n",
    "hour_start = 12  # Only files beginning after noon UTC of each day will be downloaded\n",
    "hour_end = 17  # Only files beginning before 5pm UTC will be downloaded\n",
    "bbox = [-30, 50, -20, 70]  # min_lon, min_lat, max_lon, max_lat defined bbox \n",
    "\n",
    "wue_api.download_composite(year=year, month_start=month_start, month_end=month_end, hour_start=hour_start, hour_end=hour_end, bbox=bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411163c4-5f22-4217-b5f2-d203eddbce16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
