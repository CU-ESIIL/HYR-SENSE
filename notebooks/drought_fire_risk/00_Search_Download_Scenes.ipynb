{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36554aee-a785-41e8-b73a-f37d25d18515",
   "metadata": {},
   "source": [
    "# Applications of ECOSTRESS: Drought and Wildfire Risk Assessment\n",
    "## 📍Rocky Mountain National Park, Colorado, USA\n",
    "\n",
    "STILL IN DRAFT\n",
    "\n",
    "#### Land Acknowledgement Statement\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Wildfire impacts to communities are on the rise across much of the world and especially in western North America. Increasing extreme fire behavior and societal impacts are associated with a warmer and drier climate (REF), lack of fire and a culture of fire suppression in fire-prone regions (REF), and rapid development in flammable landscapes (REF). \n",
    "\n",
    "The Wildland Urban Interface (WUI), where homes and building intermix with wildland fuels, is the fastest growing land use type in the US with XXXM more people living in the WUI since XXXX leading to increasing wildfire-related impacts (REF). While many regions of the western US are at high risk, the Colorado Front Range is particularly vulnerable/susceptible given the rapidly growing population and impacts from changing climatic patterns leading to more extreme fire behavior (REF). \n",
    "\n",
    "The 2020 fire season was record-setting in terms of area burned globally (REF). In CO it produced three of the state's top five largest wildfires. In particular, the Cameron Peak and East Troublesome fires both resulted in extreme fire behavior and hundreds of destroyed structures on the Front Range.\n",
    "\n",
    "Satellite earth observation (EO) has contributed to major advances in understanding fire risk and impacts in recent decades. Characterization of vegetative fuel using multispectral and radar imagery has changed the way we model fire behavior. Active mapping of wildfire hotspots and daily growth has improved firefighter safety and led to a better understanding of the mechanisims which drive fire growth. Future advances in tracking fuel conditions in near-real-time will improve our ability to understand and map fire risk for communities and fire management personnel.\n",
    "\n",
    "The ECOSTRESS mission provides thermal remote sensing and derivatives such as evapotranspiration. These metrics provide crucial information on the conditions of vegetation fuels on the landscape. Previous research has shown relationships between the intensity and severity of a wildfire and the water stress in plants measured in the months before wildfire: https://ecostress.jpl.nasa.gov/news/111cnasa-data-on-plant-2018sweating-could-help-predict-wildfire-severity\n",
    "\n",
    "In this case study, we will use ECOSTRESS data on Land Surface Temperature (LST) and derived Evaporative Stress Index (ESI) to assess wildfire risk in a populated area leading up to the extreme 2020 wildfires. Our focus will be on the areas around Rocky Mountain National Park (RMNP) which include the towns of Estes Park and Grand Lake, CO.\n",
    "\n",
    "Goals for this notebook:\n",
    "\n",
    "- Search for ECOSTRESS data using 'earthaccess' and a region of interest\n",
    "- Use the cloud cover product to identify granules with low cloud cover\n",
    "- Download the data granules for further analysis\n",
    "\n",
    "Citations:\n",
    "\n",
    "1.\n",
    "2.\n",
    "\n",
    "Data Sources:\n",
    "- Colorado Fire Protection Districts:\n",
    "- Rocky Mountain National Park boundary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbabd0e-7801-4c48-82c2-1bcf638614da",
   "metadata": {},
   "source": [
    "## Search and download ECOSTRESS data with 'earthaccess'\n",
    "\n",
    "In this notebook, we will use the 'earthaccess' Python package to search and download ECOSTRESS data products which overlap the Park. For more background and detailed examples of using 'earthaccess', refer to the introductory notebook from earlier in the workshop (link to intro notebook). Using this package, we can search for ECOSTRESS granules using our region of interest and identify ones which meet certain criteria for analysis.\n",
    "\n",
    "### Step 1. Setup the Notebook\n",
    "\n",
    "First, we need to import our Python packages and define some environment variable like coordinate reference systems and file paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4bc0a-307b-4777-9c72-7fb9990757b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the necessary packages and set environment variables\n",
    "\"\"\"\n",
    "\n",
    "# Import packages\n",
    "import os, shutil, time\n",
    "import datetime\n",
    "import folium\n",
    "import earthaccess\n",
    "import warnings\n",
    "import folium.plugins\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "import math\n",
    "\n",
    "from osgeo import gdal\n",
    "from branca.element import Figure\n",
    "from IPython.display import display\n",
    "from shapely import geometry\n",
    "from skimage import io\n",
    "from datetime import timedelta\n",
    "from shapely.geometry.polygon import orient\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Projection information\n",
    "geog = 'EPSG:4326'  # Geographic projection\n",
    "prj = 'EPSG:5070'  # Projected coordinate system- WGS 84 NAD83 UTM Zone 13N\n",
    "\n",
    "# File path information\n",
    "datadir = '/data-store/iplant/home/shared/esiil/HYR_SENSE/'\n",
    "\n",
    "# File path information\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82919ccc-e8ec-49f8-97b4-f0bcd633eed5",
   "metadata": {},
   "source": [
    "#### Define Custom Functions\n",
    "\n",
    "These helper functions will be used throughout the notebook. For each function, there is a description of what purpose it serves and where it is used in the notebook. Feel free to read through each function to get an idea of what it does, but we will not cover these in great detail. Ask a facilitator if you have any questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca425f2b-7aa6-4e1e-a487-38403b35ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapely_object(result:earthaccess.results.DataGranule):\n",
    "    \"\"\"\n",
    "    Retrieve geospatial information from ECOSTRESS granule footprints.\n",
    "    This function allows us to retrieve the geographic coverage for each granule and plot it on a map.\n",
    "    \n",
    "    :param 'result:earthaccess.results.DataGranule': a single data granule from earthaccess data search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get Geometry Keys\n",
    "    geo = result['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry']\n",
    "    keys = geo.keys()\n",
    "\n",
    "    if 'BoundingRectangles' in keys:\n",
    "        bounding_rectangle = geo['BoundingRectangles'][0]\n",
    "        # Create bbox tuple\n",
    "        bbox_coords = (bounding_rectangle['WestBoundingCoordinate'],bounding_rectangle['SouthBoundingCoordinate'],\n",
    "                       bounding_rectangle['EastBoundingCoordinate'],bounding_rectangle['NorthBoundingCoordinate'])\n",
    "        # Create shapely geometry from bbox\n",
    "        shape = geometry.box(*bbox_coords, ccw=True)\n",
    "    elif 'GPolygons' in keys:\n",
    "        points = geo['GPolygons'][0]['Boundary']['Points']\n",
    "        # Create shapely geometry from polygons\n",
    "        shape = geometry.Polygon([[p['Longitude'],p['Latitude']] for p in points])\n",
    "    else:\n",
    "         raise ValueError('Provided result does not contain bounding boxes/polygons or is incompatible.')\n",
    "    return(shape)\n",
    "\n",
    "\n",
    "def get_png(result:earthaccess.results.DataGranule):\n",
    "    \"\"\"\n",
    "    Retrieve the browse image from the search results\n",
    "    This browse image can be used to display the granule quickly and effectively before downloading\n",
    "\n",
    "    :param 'result:earthaccess.results.DataGranule': a single data granule from earthaccess data search\n",
    "    \"\"\"\n",
    "    https_links = [link for link in result.dataviz_links() if 'https' in link]\n",
    "    if len(https_links) == 1:\n",
    "        browse = https_links[0]\n",
    "    elif len(https_links) == 0:\n",
    "        browse = 'no browse image'\n",
    "        warnings.warn(f\"There is no browse imagery for {result['umm']['GranuleUR']}.\")\n",
    "    else:\n",
    "        browse = [png for png in https_links if '.png' in png][0]\n",
    "    return(browse)\n",
    "\n",
    "\n",
    "def convert_bounds(bbox, invert_y=False):\n",
    "    \"\"\"\n",
    "    Helper method for changing bounding box representation to leaflet notation\n",
    "    Leaflet interactive maps require a specific format for coordinates, this function sets that up for a given bounding box.\n",
    "\n",
    "    ``(lon1, lat1, lon2, lat2) -> ((lat1, lon1), (lat2, lon2))``\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    if invert_y:\n",
    "        y1, y2 = y2, y1\n",
    "    return ((y1, x1), (y2, x2))\n",
    "\n",
    "\n",
    "def extract_granule_id(link):\n",
    "    return os.path.basename(link)[:-3]\n",
    "    \n",
    "print(\"Functions are ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f520f271-a7ea-4a56-9106-8a07186633fc",
   "metadata": {},
   "source": [
    "### Step 2. Data Preparation\n",
    "\n",
    "#### Accessing data from the CyVerse Data Store\n",
    "\n",
    "The CyVerse \"data store\" contains some geospatial and tabular data for the workshop. These data can be accessed directly using file paths. However, this sometimes can cause delays. To avoid these issues, we can copy the data for this notebook into a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1a2b5-714a-4635-b29f-1ba6191aae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copy the data-store to 'local' directory\n",
    "This enables quick access to data files\n",
    "\"\"\"\n",
    "\n",
    "# Identify the location of the HYR-SENSE \"data store\"\n",
    "data_store_path = '/data-store/iplant/home/shared/esiil/HYR_SENSE/data/Drought-FireRisk'\n",
    "# Set a destination path (this is a 'local' and temporary path)\n",
    "dest = '/home/jovyan/HYR-SENSE/data/Drought-FireRisk/' # in the GitHub repo we cloned\n",
    "if not os.path.exists(dest):\n",
    "    os.mkdir(dest) # create the directory for the copied data, if needed\n",
    "    \n",
    "# Using 'shutil' package, copy all the files over\n",
    "shutil.copytree(data_store_path, dest, dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78560d4-c6b9-4efb-afbd-ba9186c6445c",
   "metadata": {},
   "source": [
    "#### Importing project data\n",
    "\n",
    "Now that we have a local copy of our project data, we can read in some of the data files like our region of interest (ROI). For this notebook, we will load three spatial datasets to set the stage and search for ECOSTRESS data:\n",
    "\n",
    "- Rocky Mountain National Park (RMNP)\n",
    "- Colorado Fire Protection Districts (FPDs)\n",
    "- CO Wildfire Perimeters\n",
    "\n",
    "Fire protection districts (FPDs) are essentially service areas for communities at risk to wildfire. They help define regions of potential higher risk where people and wildland fuels often intermingle. \n",
    "\n",
    "To begin, let's create a map of our ROI, the intersecting FPDs and the 2020 wildfire burn perimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88abd4-9523-432c-b79f-56b1261ac502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load the RMNP boundary as a GeoJSON using the Python package GeoPandas\n",
    "rmnp = gpd.read_file(os.path.join(dest, 'NPS_ROMO_Boundary.geojson'))\n",
    "rmnp = rmnp.to_crs(prj) # projected CRS\n",
    "\n",
    "### Load the Colorado Fire Protection Districts\n",
    "## Intersect with the RMNP boundary\n",
    "fpd = gpd.read_file(os.path.join(dest, 'CO_Fire_Protection_Districts.geojson'))\n",
    "fpd = fpd.to_crs(prj) # projected crs\n",
    "fpd_rmnp = fpd[fpd.intersects(rmnp.unary_union)] # Perform the spatial intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7323c1cf-c71d-4ee8-bef3-619d2328ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the interactive map using folium\n",
    "fig = Figure(width=\"850px\", height=\"375px\")\n",
    "map1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\n",
    "fig.add_child(map1)\n",
    "\n",
    "rmnp.explore(\n",
    "    \"PARKNAME\",\n",
    "    categorical=True,\n",
    "    tooltip=[\n",
    "        \"PARKNAME\",\n",
    "    ],\n",
    "    popup=True,\n",
    "    style_kwds=dict(fillOpacity=0.81, width=5),\n",
    "    name=\"RMNP\",\n",
    "    m=map1,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "fpd_rmnp.explore(\n",
    "    \"lgname\",\n",
    "    tooltip=[\n",
    "        \"lgname\",\n",
    "    ],\n",
    "    popup=True,\n",
    "    style_kwds=dict(fillOpacity=0.2, color=\"black\", width=2),\n",
    "    name=\"FPD\",\n",
    "    m=map1,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "map1.fit_bounds(map1.get_bounds(), padding=(30, 30))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40ce52-3e9b-4887-8191-0000a6d79e6a",
   "metadata": {},
   "source": [
    "### Step 3. Search for ECOSTRESS Data \n",
    " - Land Surface Temperature and Emissivity Daily L2 Global 70 m\n",
    " - Evaporative Stress Index (ESI) PT-JPL Daily L4 Gloabl 70 m \n",
    "\n",
    "Now that we have our ROI loaded, we can use the 'earthaccess' Python package to find ECOSTRESS data products. In particular, we will search for the Land Surface Temperature and Emissivity Daily Gridded L2 70-m and the Evaporative Stress Index Daily L4 70 m products. With earthaccess, we can search for and print all of the available data sets. Using this capability, we can also search for a specific data product by its \"short name\". To start, let's get a list of all the ECOSTRESS products available.\n",
    "\n",
    "Read more about the data products:\\\n",
    "ECOSTRESS LSTE: https://lpdaac.usgs.gov/products/eco2lstev001/ \\\n",
    "ECOSTRESS ESI: https://lpdaac.usgs.gov/products/eco4esiptjplv001/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ee360-1bf0-4581-b279-d4f51519836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to your NASA Earthdata\n",
    "# You only have to do this once in your session (persist=True saves the credentials)\n",
    "auth = earthaccess.login(persist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a89af0-d4af-43a8-8caa-577a7bd822dd",
   "metadata": {},
   "source": [
    "#### Search for available ECOSTRESS products\n",
    "\n",
    "To begin, we can identify a list of available products. The \"ShortName\" attribute may not give enough description, so some initial work to identify which data products you are interested in is helpful. There are currently two data versions available.\n",
    "\n",
    "In our case we are looking for LST and ESI. Because we are searching for data for the 2020 wildfire season, we may not have any Version 002 data available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864101d-ce80-4e5c-a3fd-3b66093a620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query the collections for ECOSTRESS\n",
    "Query = earthaccess.collection_query().keyword('ECOSTRESS')\n",
    "print(f'Collections found: {Query.hits()}')\n",
    "\n",
    "# Return search results as a list\n",
    "collections = Query.fields(['ShortName']).get(Query.hits())\n",
    "\n",
    "# Retrieve Collection short-names\n",
    "print(\"All available data products: \")\n",
    "[product['short-name'] for product in [collection.summary() for collection in collections]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb426f-fd3f-4957-af86-060a662e1bad",
   "metadata": {},
   "source": [
    "#### Define a search request using 'earthaccess'\n",
    "\n",
    "We are ready to submit a search request for our ECOSTRESS data over RMNP. We want to find data to assess the drought conditions and wildfire risk during the 2020 fire season and leading up to the start of both the East Troublesome and Cameron Peak wildfires. So, we will search for both LST and ESI from mid-June to mid-August, prior to the ignition of these fires. \n",
    "\n",
    "Because we are working with data from 2020, we may not be able to find adequate Version 002 (V002) ECOSTRESS data products, which are being processed now. Eventually, the V002 product will supercede the V001. However, in the short term you may find that there is only V001 data available in your region of interest as is the case in this example. \n",
    "\n",
    "You can search for V002 data using the alternative \"ShortName\" lists in the code below. There are slightly different workflows for V001 and V002 as you will see throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ea005-0ecd-4fdc-8f78-e62a4c894c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define some search parameters \n",
    "# Version 001: ['ECO2LSTE', 'ECO4ESIPTJPL']\n",
    "# Version 002: ['ECO_L2T_LSTE', 'ECO_L4T_ESI']\n",
    "short_names =  ['ECO2LSTE', 'ECO4ESIPTJPL']\n",
    "date_range = ('2020-06-15','2020-08-15') # June15-August15 2020\n",
    "\n",
    "# 2. Retrieve the coordinate pairs for the region of interest\n",
    "geom = rmnp.geometry.to_crs(geog) # convert back to geographic coordinates\n",
    "geom = geom.unary_union.envelope # dissolve into a single shape and get the envelope\n",
    "coords = list(geom.exterior.coords) # retrieve the coordinate pairs\n",
    "\n",
    "# 3. Loop through each short name, create a new search\n",
    "# Looping here helps us see how many tiles exist for each product\n",
    "results = {} # empty dictionary to store the search results for each product\n",
    "for short_name in short_names:\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(f\"Data product: {short_name}\")\n",
    "    \n",
    "    # 3a. Search for ECOSTRESS products matching our short name\n",
    "    result = earthaccess.search_data(\n",
    "        short_name=short_name,\n",
    "        polygon=coords,\n",
    "        temporal=date_range,\n",
    "        count=500, \n",
    "    )\n",
    "        \n",
    "    # 3b. Extract the search results as a data frame\n",
    "    df = pd.json_normalize(result)\n",
    "    df['shortname'] = short_name\n",
    "\n",
    "    # 3c. Create shapely polygons for result (granule footprints)\n",
    "    geometries = [get_shapely_object(result[index]) for index in df.index.to_list()]\n",
    "    # Convert to GeoDataframe\n",
    "    results_gdf = gpd.GeoDataFrame(df, geometry=geometries, crs=\"EPSG:4326\")\n",
    "\n",
    "    # 3d. Add browse imagery links (we will use this later)\n",
    "    results_gdf['browse'] = [get_png(granule) for granule in result]\n",
    "    \n",
    "    # 3e. Retrieve the data links (used later for downloading)\n",
    "    data_links = pd.DataFrame([granule.data_links()[0] for granule in result], columns=['data_link']) \n",
    "    results_gdf = pd.concat([results_gdf, data_links], axis=1)  # join the data links back to our search results\n",
    "\n",
    "    # 3f. Tidy the data columns\n",
    "    # Rename some of the columns for easier access\n",
    "    results_gdf = results_gdf.copy()\n",
    "    results_gdf.rename(\n",
    "        columns = {'meta.concept-id':'concept_id',\n",
    "                   'meta.native-id':'granule',\n",
    "                   'umm.TemporalExtent.RangeDateTime.BeginningDateTime':'start_datetime',\n",
    "                   'umm.TemporalExtent.RangeDateTime.EndingDateTime':'end_datetime',\n",
    "                   'umm.DataGranule.DayNightFlag':'day_night',\n",
    "                   'umm.CloudCover':'cloud_cover',\n",
    "                  }, inplace=True)\n",
    "    # Tidy up the date field and extract the time of day\n",
    "    results_gdf['datetime_obj'] = pd.to_datetime(results_gdf['start_datetime']) \n",
    "    results_gdf['time_of_day'] = results_gdf['datetime_obj'].dt.time\n",
    "\n",
    "    results_gdf['identifier'] = results_gdf['umm.DataGranule.Identifiers'].apply(lambda x: x[0]['Identifier'])\n",
    "\n",
    "     # 3f. Filter for daytime observations\n",
    "    results_gdf = results_gdf[results_gdf['day_night'].str.contains('Day')]\n",
    "    print(f'Daytime observations: {len(results_gdf)}')\n",
    "    \n",
    "    # Add to the dictionary\n",
    "    results[short_name] = results_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7b900-c7e6-4e6f-a668-4271251064e3",
   "metadata": {},
   "source": [
    "#### Examine the metadata from the search results  \n",
    "It looks like we have some options for V1 data products during this spatial and temporal window. However, to identify data granules which will be useful for analysis, we need to dig into the metadata a bit more. Thankfully, the earthaccess enables us to work with the metadata from our search results as a data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41174038-cce6-42dd-ad93-f4267584360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the columns available in the metadata\n",
    "for key, value in results.items():\n",
    "    print(key)\n",
    "    print(f'{len(value)} granules total.')\n",
    "    print(value.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d911a2-d66f-4d61-a36f-5673d6a3c735",
   "metadata": {},
   "source": [
    "#### Keep only granules which fully contain our study region\n",
    "\n",
    "Using the footprint spatial information, we can remove granules from our list which do not fully contain our study region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f0bda-3d6c-42ed-9be2-70ba389b1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a bounding box fully contains the romo region\n",
    "def bbox_contains(bbox_geom, roi_geom):\n",
    "    return bbox_geom.contains(roi_geom.unary_union)\n",
    "\n",
    "# Filter results based on spatial containment\n",
    "results_rmnp = {}\n",
    "for key, gdf in results.items():\n",
    "    print(f'{key}: {len(value)} granules total.')\n",
    "    # Check if the granule completely contains the bounding box of our study region\n",
    "    filtered = gdf[gdf['geometry'].apply(lambda x: bbox_contains(x, rmnp.geometry.to_crs(geog)))]\n",
    "    print(f'Removed {len(gdf)-len(filtered)} granules with partial overlap. \\n ({len(filtered)} total)')\n",
    "    \n",
    "    # Append the filtered results back to the new dictionary\n",
    "    results_rmnp[key] = filtered\n",
    "\n",
    "# Examine the data frame for one product\n",
    "results_rmnp[short_names[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef129d6-4302-4755-bcc9-0dd91bf7b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of granules for each time period\n",
    "for key, gdf in results_rmnp.items():\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.hist(gdf['datetime_obj'], bins=10, color='green', edgecolor='black', linewidth=1)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title(f'Histogram of Granules for {key}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Granules')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59207d38-556e-4a04-812f-0244beac55e7",
   "metadata": {},
   "source": [
    "#### Temporal filtering\n",
    "\n",
    "##### 1. Ensure common acquisition dates between both products\n",
    "\n",
    "We want to relate LSTE to ESI to try and understand drought-induced fire risk in our study area. So, we need to make sure we can identify granules with the same acquisition date between the two products. Generally, this should not be necessary as the LST L2 product should have the same L4 derivatives. However, if you are using different products or want to search for common dates between ECOSTRESS and EMIT, for example, this is a useful application.\n",
    "\n",
    "##### 2. Retain granules which were collected during the middle of the day (10am-3pm) for consistency\n",
    "\n",
    "The time of day of the imagery collection matters greatly for things like evaporative stress and land surface temprature. To keep things consistent, we can check for granules which were collected between 10am-3pm using the date information in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465089a-3a30-41ac-afe4-7d65d824accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify matching dates between the two products\n",
    "set1 = set(results_rmnp[short_names[0]]['start_datetime'])\n",
    "set2 = set(results_rmnp[short_names[1]]['start_datetime'])\n",
    "common_dates = set1.intersection(set2)\n",
    "print(common_dates)\n",
    "\n",
    "# Define our daily temporal window\n",
    "start_time = datetime.time(10, 0, 0)  # 10:00 AM\n",
    "end_time = datetime.time(15, 0, 0)    # 3:00 PM\n",
    "\n",
    "# Keep matching dates\n",
    "for key, gdf in results_rmnp.items():\n",
    "    # Filter to retain common dates\n",
    "    \n",
    "    filtered = gdf[gdf['start_datetime'].isin(common_dates) & \n",
    "        (gdf['time_of_day'] >= start_time) & \n",
    "        (gdf['time_of_day'] <= end_time)]\n",
    "    \n",
    "    # Replace the data frame in the dictionary\n",
    "    results_rmnp[key] = filtered.reset_index()\n",
    "print(\"----------------------\")\n",
    "\n",
    "print(f'{len(results_rmnp[short_names[0]])} matching granules.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d498b08-47aa-436f-9695-34af6ae541ea",
   "metadata": {},
   "source": [
    "#### Retrieve cloud cover and geolocation for our filtered granules\n",
    "\n",
    "We have narrowed our search results down to a set of potential dates. Because we are working with the V1 raw data, we will need two additional products to move forward with analysis: the cloud cover and geolocation products. This will allow us to identify cloudy pixels and create a georeferenced image from the data granules.\n",
    "\n",
    "Since we are working with the ECOSTRESS V1 data, the data products we need to retrieve are the \"ECO2CLD\" and \"ECO1BGEO\". We can use 'earthaccess' again to search for these products for our filtered granules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa0077-966e-4706-8917-ffa41dfc0f47",
   "metadata": {},
   "source": [
    "#### Visualize the search results\n",
    "\n",
    "Using the granule footprints, we can make an interactive map showing the granules. We can also create a static map with the granule browse images to help identify quality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51f849-a574-4964-8034-4a3b121e9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive map with Folium\n",
    "\n",
    "fig = Figure(width=\"750px\", height=\"375px\")\n",
    "map1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\n",
    "fig.add_child(map1)\n",
    "\n",
    "# Let's add only the LSTE granules\n",
    "gdf = results_rmnp[short_names[0]]\n",
    "\n",
    "# Plot Region of Interest\n",
    "rmnp.explore(\n",
    "    popup=False,\n",
    "    style_kwds=dict(fillOpacity=0.1, width=2),\n",
    "    name=\"Region of Interest\",\n",
    "    m=map1\n",
    ")\n",
    "\n",
    "# Plot the granule footprints\n",
    "gdf.drop(columns=['datetime_obj', 'time_of_day']).explore(\n",
    "    \"granule\",\n",
    "    categorical=True,\n",
    "    tooltip=[\n",
    "        \"granule\",\n",
    "        \"start_datetime\",\n",
    "    ],\n",
    "    popup=True,\n",
    "    style_kwds=dict(fillOpacity=0.1, width=2),\n",
    "    name=\"ECOSTRESS\",\n",
    "    m=map1,\n",
    "    legend=False\n",
    ")\n",
    "    \n",
    "map1.fit_bounds(bounds=convert_bounds(results_gdf.unary_union.bounds))\n",
    "map1.add_child(folium.LayerControl())\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d3838-068f-4da6-ae7e-1a8b88d57f44",
   "metadata": {},
   "source": [
    "With so many overlapping granules, the interactive map is not necessarily the most useful. Let's see if we can identify granules with quality data using the browse images. It may also be helpful to examine the \"browse images\" which provide a snapshot of the data coverage and can help us identify possible granules for analysis. In the code block below, we use the ECOSTRESS LSTE product browse images to identify granules which may have adequate data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58e440-8478-4a1b-ab8d-d91a7132904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the browse images to check for quality granules\n",
    "# Let's do this just for LSTE as we hope the quality days will be the same for both products\n",
    "    \n",
    "short_name = short_names[0] # this is our LST short name identified earlier\n",
    "gdf = results_rmnp[short_names[0]] # this is the search result dictionary for LST\n",
    "\n",
    "browse_urls = gdf['browse'].tolist()\n",
    "\n",
    "cols = 3\n",
    "rows = math.ceil(len(gdf) / cols)\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(12,12))\n",
    "ax = ax.flatten()\n",
    "\n",
    "for _n, (index, row) in enumerate(gdf.iterrows()):\n",
    "    try:\n",
    "        img = io.imread(row['browse'])\n",
    "        ax[_n].imshow(img)\n",
    "        ax[_n].set_title(f\"Index: {index} - {row['start_datetime']}\")\n",
    "        ax[_n].axis('off')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load image at {row['browse']}: {e}\")\n",
    "        ax[_n].axis('off')\n",
    "        ax[_n].text(0.5, 0.5, 'Image not available', ha='center', va='center')\n",
    "\n",
    "# Turn off any remaining empty subplots\n",
    "for i in range(_n + 1, len(ax)):\n",
    "    ax[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0adf9-23d6-47b0-ba7c-fdbd789c820f",
   "metadata": {},
   "source": [
    "Notice that it can still be quite difficult to interpret the browse images. These images are not georeferenced, so we cannot place them in relationship to our region of interest. Using a combination of this static map and the interactive opne we plotted before can you help narrow down. The V001 ECOSTRESS product also has a cloud cover mask which can be used to determine how much cloud cover is in a given granule (https://data.nasa.gov/dataset/ECOSTRESS-Cloud-Mask-Daily-L2-Global-70m-V001/xvz7-452n/about_data).\n",
    "\n",
    "For the purpose of the exercise, we have identified two granules which look like they have quality data over our region of interest. In the next section, we will download these granules and their associated geolocation information (https://cmr.earthdata.nasa.gov/search/concepts/C1534584923-LPDAAC_ECS.html).\n",
    "\n",
    "In the code below, we can use the index in the static map to select our quality granules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebfb542-9a5a-4fb2-b863-ecc9bb862f98",
   "metadata": {},
   "source": [
    "#### Download or stream granules\n",
    "\n",
    "Now we have a list of data links including the geolocation information. We can download these data to a local directory. Based on the browse images above, we can select a couple granules which look like they have quality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278783c-97b9-4e18-82b2-6f7911f78504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select the quality granules based on their index\n",
    "good_dates = [4,10]\n",
    "for short_name, gdf in results_rmnp.items():\n",
    "    # Subset based on the index\n",
    "    gdf_ = gdf[gdf.index.isin(good_dates)]\n",
    "    print(\"Downloading selected granules:\\n\")\n",
    "    print_indices(gdf_,short_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee88652-4303-41a4-90e3-87b5c0b6ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time() # begin time\n",
    "\n",
    "# Function to print indices and dates\n",
    "def print_indices(gdf, short_name):\n",
    "    print(f\"Indices and dates for {short_name}:\")\n",
    "    for index, row in gdf.iterrows():\n",
    "        print(f\"Index: {index}, Date: {row['start_datetime']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Get Https Session using Earthdata Login Info\n",
    "fs = earthaccess.get_requests_https_session()\n",
    "\n",
    "# Define the output directory\n",
    "dest = '/home/jovyan/HYR-SENSE/data/Drought-FireRisk/' \n",
    "\n",
    "# Select the quality granules based on their index\n",
    "good_dates = [4,10]\n",
    "# Loop through earch search result data frame, retrieve the data links, and download the files\n",
    "for short_name, gdf in results_rmnp.items():\n",
    "\n",
    "    # Get the good data products\n",
    "    gdf = gdf[gdf.index.isin(good_dates)]\n",
    "\n",
    "    print(\"Downloading selected granules:\\n\")\n",
    "    print_indices(gdf_,short_name)\n",
    "\n",
    "    # Grab the download URLs\n",
    "    data_links = [data_link for data_link in gdf['data_link']]\n",
    "\n",
    "    # Download the data granules    \n",
    "    for url in data_links:\n",
    "        # Retrieve the granule ID from the download URL\n",
    "        granule_id = os.path.basename(url)\n",
    "        print(f\"Granule ID: {granule_id}\")\n",
    "\n",
    "        # Set the output path for the downloaded files\n",
    "        out_path = os.path.join(dest,f'{short_name}')\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path) # Check if the folder exists, create if not\n",
    "        fp = os.path.join(out_path,f'{granule_id}')\n",
    "    \n",
    "        # Download the Granule Asset if it doesn't exist\n",
    "        if not os.path.isfile(fp):\n",
    "            with fs.get(url,stream=True) as src:\n",
    "                with open(fp,'wb') as dst:\n",
    "                    for chunk in src.iter_content(chunk_size=64*1024*1024):\n",
    "                        dst.write(chunk)\n",
    "        else:\n",
    "            print(\"*** Already downloaded ***\")\n",
    "            print(\" \")\n",
    "    \n",
    "        print('Time to complete granule:', time.time() - t0)\n",
    "\n",
    "print('Total time:', time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4e4b9-4caf-41eb-afb7-16f8fe1ae7ba",
   "metadata": {},
   "source": [
    "### Download the geolocation information\n",
    "\n",
    "Because we are working with the V1 data, we also need to pull the geolocation information for these granules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b78b0-1372-44fb-970b-d3ed342c0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the geolocation identifier\n",
    "geo_id = \"ECOSTRESS_L1B_GEO_\"\n",
    "short_name = 'ECO1BGEO'\n",
    "\n",
    "# Select indices which have good data\n",
    "good_dates = [4,10]\n",
    "\n",
    "# Use the LST granules to find the correct georlocation information\n",
    "lst = results_rmnp[short_names[0]]\n",
    "lst_selected = lst[lst.index.isin(good_dates)]\n",
    "\n",
    "# Retrieve a list of the identifiers\n",
    "selected_granules = []\n",
    "for identifier in lst_selected['identifier']:\n",
    "    print(identifier)\n",
    "    # Split the string by underscore\n",
    "    parts = identifier.split('_')\n",
    "    # Join the parts from the fourth element to the end to get the desired string\n",
    "    selected_granules.append(geo_id+'_'.join(parts[3:]))\n",
    "print(selected_granules)\n",
    "\n",
    "# Search for the geolocation information and filter based on the 'identifier' that we selected\n",
    "result = earthaccess.search_data(\n",
    "    short_name=short_name,\n",
    "    polygon=coords,\n",
    "    temporal=date_range,\n",
    "    count=500, \n",
    ")\n",
    "\n",
    "# Grab the URL for download\n",
    "# Extract the search results as a data frame\n",
    "df = pd.json_normalize(result)\n",
    "links = [granule.data_links()[0] for granule in result]\n",
    "\n",
    "# Filter the full pathnames by checking for partial matches with base names\n",
    "geo_links = [link for link in links if any(base in link for base in selected_granules)]\n",
    "print(geo_links)\n",
    "\n",
    "# Download the geolocation data\n",
    "for url in geo_links:\n",
    "    granule_id = os.path.basename(url)\n",
    "    print(f\"Downloading granule ID: {granule_id}\")\n",
    "    \n",
    "    out_path = os.path.join(dest,f'{short_name}')\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "    fp = os.path.join(out_path,f'{granule_id}')\n",
    "    print(fp)\n",
    "    # Download the Granule Asset if it doesn't exist\n",
    "    if not os.path.isfile(fp):\n",
    "        with fs.get(geo_link[0],stream=True) as src:\n",
    "            with open(fp,'wb') as dst:\n",
    "                for chunk in src.iter_content(chunk_size=64*1024*1024):\n",
    "                    dst.write(chunk)\n",
    "    else:\n",
    "        print(\"*** Already downloaded ***\")\n",
    "        print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyr-sense",
   "language": "python",
   "name": "hyr-sense"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
