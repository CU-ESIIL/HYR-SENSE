{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a27fa42-7be7-4d5c-9371-24afc9089d16",
   "metadata": {},
   "source": [
    "# Introduction to NASA `earthaccess`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea388888-13ed-4959-a88d-5528e97e39b2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates how to search for ECOSTRESS version 2 data collections from NASA's Earthdata Cloud using the [`earthaccess`](https://github.com/nsidc/earthaccess) package. `earthaccess` is a python library to search, download, or stream NASA Earth science data with just a few lines of code. The library abstracts the [NASA Common Metadata Repository (CMR) API](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html), manages authentication, and enables reproducible programmatic search and access for both DAAC-hosted (on-prem) and cloud-hosted (Earthdata Cloud) data.  \n",
    "\n",
    "A NASA Earthdata Login account ([EDL](https://urs.earthdata.nasa.gov/profile)) is required to download or access data. Earthdata Login accounts are free and can be set up in only a few minutes. Remember your EDL username and password as they are needed for authentication in this and other data access resources.\n",
    "\n",
    "> **Note** Generally speaking we do not need authentication for querying collections and granules unless they are restricted datasets for early adopters.  \n",
    "\n",
    "## Requirements  \n",
    "\n",
    "- A NASA [Earthdata Login](https://urs.earthdata.nasa.gov/) account is required   \n",
    "\n",
    "## Learning Objectives  \n",
    "\n",
    "- How to get information about data collections using `earthaccess`\n",
    "- How to query for data using spatiotemporal parameters\n",
    "- How to work with `earthaccess` request objects|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba2387-54e8-4aa5-aedb-c7d90644536f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise  \n",
    "\n",
    "Let's start by loading in the needed packages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6c9ed-fe58-4e03-b29b-c6c447061f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import os\n",
    "import geopandas as gp\n",
    "import hvplot.pandas\n",
    "\n",
    "# define PROJ_LIB required by PROJ library manually. see details at https://gis.stackexchange.com/questions/364421/how-to-make-proj-work-via-anaconda-in-google-colab/370360#370360 \n",
    "os.environ['PROJ_LIB'] = '/opt/conda/envs/hyr-sense/share/proj'\n",
    "\n",
    "earthaccess.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95121ff7-5222-4778-a4de-25625e23884b",
   "metadata": {},
   "source": [
    "`earthaccess` provides 3 different \"strategies\" to authenticate with NASA EDL.\n",
    "\n",
    "* **netrc**: Do we have a **.netrc** file with our EDL credentials? if so, we can use it with `earthaccess`.\n",
    "If we don't have it and want to create one we can. `earthaccess` allows users to type their credentials and persist them into a .netrc file.\n",
    "* **environment**: If we have our EDL credentials as environment variables \n",
    "  * EDL_USERNAME\n",
    "  * EDL_PASSWORD\n",
    "* **interactive**: We will be asked for our EDL credentials with optional persistance to .netrc\n",
    "\n",
    "The below code with cycle through the strategies and automatically persist a **.netrc** file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login()\n",
    "# are we authenticated?\n",
    "print(auth.authenticated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e73f7-21f1-42ad-a446-2dc4e640beb5",
   "metadata": {},
   "source": [
    "`earthaccess` creates and leverages Earthdata Login tokens to authenticate with NASA systems. Earthdata Login tokens expire after a month and will no longer work when trying to download or stream data using `earthaccess`. Use the `refresh_tokens()` to generate a new token for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a5f73d-b4b9-4dad-ac72-12c7ae6278d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#auth.refresh_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8b461-8c68-4719-94e5-34057159dac7",
   "metadata": {},
   "source": [
    "## Querying for datasets, AKA collections\n",
    "\n",
    "We need information about the data collection we're interested in before we can find the data granules we would like to process. We'll use the `search_datasets()` function to query for collections that match our input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889b3fb-9c86-4de3-a5cf-ec53fb9fb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_req = earthaccess.search_datasets(\n",
    "    provider='LPCLOUD',    # LPCLOUD is the LP DAAC Archive in Earthdata Cloud\n",
    "    keyword='ecostress',\n",
    "    version='002'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db2b2d-2d03-4acc-86aa-d1f498ad800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'collections_req is a {type(collections_req)} of {type(collections_req[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26962664-cbe8-453f-b617-80d473df9c75",
   "metadata": {},
   "source": [
    "Queries return a list of `earthaccess` `DataCollections`. `earthaccess` `DataCollections` are enhanced python dictionaries and as such, can be interacted with like any Python dictionary. Let's take a look at the first collection in `collections_req`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f390eec-6cf8-4c7f-b850-8d61f5755757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collection = collections_req[0]    # Get the first earthaccess DataCollection in the list\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18354eef-d687-4ab2-944d-dc1f20322c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a153bc19-b07d-4bf4-9daf-264546026b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection['umm'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5154c-f131-44ad-a68f-cf0fa21ce18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection['umm']['ShortName']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d45a6f-ac37-4744-bcfe-88ac3dd6ac07",
   "metadata": {},
   "source": [
    "The `DataCollections` class also has some handy helper methods.\n",
    "\n",
    "```python \n",
    "collection.concept_id() # returns the concept-id, used to search for data granules\n",
    "collection.abstract() # returns the abstract\n",
    "collection.landing_page() # returns the landing page if present in the UMM fields\n",
    "collection.get_data() # returns the portal where data can be accessed.\n",
    "```\n",
    "\n",
    "The same results can be obtained using the `dict` syntax:\n",
    "\n",
    "```python\n",
    "collection[\"meta\"][\"concept-id\"] # concept-id\n",
    "collection[\"umm\"][\"RelatedUrls\"] # URLs, with GET DATA, LANDING PAGE etc\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07003f4b-ae18-4175-aad4-c2e99b1f3eb1",
   "metadata": {},
   "source": [
    "Another helpful method is `summary()`. This method prints some of the more common collection metadata information used for additional queries against the individual collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318d9c5-107b-4338-b0e5-eb373481976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78afe0-04e5-4e76-9b6c-2b668e55fd4c",
   "metadata": {},
   "source": [
    "The most common way to query for information or metadata related to a collection is to use the **concept-id**, the collection **doi**, or a combination of both the **short-name** and **version**. The `summary()` method gives us the **concept-id**, **short-name**, and **version**. We can use this knowledge to create a list containing this information for later queries against the specific collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1ddc8-5478-4e3d-9239-ce29834b7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_info = [{n:[c.summary()['short-name'], c.summary()['concept-id'], c.summary()['version']]} for n, c in enumerate(collections_req)]\n",
    "collections_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77130c-4ce5-4d4b-b656-2be1fb513c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections_info[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9c3bb-ac8b-48e8-8233-8c44da8fb7bc",
   "metadata": {},
   "source": [
    "## Querying for data files (granules)\n",
    "\n",
    "We can use the collection information from above to start finding data files we want to work with. In this example, we will use the **short_name** and the **version** to query for data granules from [`ECO_L2T_LSTE`](https://doi.org/10.5067/ECOSTRESS/ECO_L2_LSTE.002) version `002` dataset. You can search data granules using just a **short_name** but there is the potential that multiple versions of the data collection will be return. To query for granules in a more explicit way, a **concept-id** would be the best option.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364d737-5a79-4089-853f-76d2ad1c85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build our query\n",
    "granules_request = earthaccess.search_data(\n",
    "    short_name='ECO_L2T_LSTE',\n",
    "    version='002',\n",
    "    provider='LPCLOUD',\n",
    "    count=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc41f61-2a9e-4883-b7e5-d4c1cd716eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'granules_request is a {type(granules_request)} of {type(granules_request[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c678a5-cb93-4986-bb3f-2a76623acdcc",
   "metadata": {},
   "source": [
    "Again, our query for granules has returned a list of python dictionaries (`earthaccess.results.DataGranule`). We can therefore access all the keys and values like we usually do with Python dictionaries.  \n",
    "\n",
    "This query returned a lot of granules. Let's refine our results using **bounding box** and **temporal** constraints.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f32a4-026d-4a5f-af66-69026cabe966",
   "metadata": {},
   "source": [
    "### Spatiotemporal queries\n",
    "\n",
    "The `earthaccess.results.DataGranule` and `earthaccess.results.DataCollection` classes accept the same spatial and temporal arguments as CMR, so we can search for granules that match spatiotemporal criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ddb145-c7dd-4a13-93bc-1e257dcde432",
   "metadata": {},
   "source": [
    "#### Specify Spatial Parameters\n",
    "\n",
    "Search queries can be refined by using spatial parameters. `earthaccess` accepts point and area arguments. For point features a longitude and latitude coordinate pair must be passes as a tuple to the `point` parameter. For example:\n",
    "\n",
    "```\n",
    "point=(-105.64788824641289,39.98286247719818)\n",
    "```\n",
    "\n",
    "For areas features, queries can leverage the `bounding_box` -- a tuple containing coordinates in the order lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat -- or `polygon` -- list of (lon, lat) tuples -- parameters. We'll use the `bounding_box` parameter in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b55f3-c2a4-4a45-9f1f-4e5fd7b10a1b",
   "metadata": {},
   "source": [
    "**Reading a geojson file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54bfc2-48e6-423d-8309-3b5fa3b9f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson = gp.read_file('../../data/NIWO_box.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7594563-2e98-4c7d-b881-50939bf787b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_plot = geojson.hvplot(tiles='ESRI', color='yellow', alpha=0.5, crs='EPSG:4326')\n",
    "geojson_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d068e1d7-3d85-442d-9c4b-9df963e646ce",
   "metadata": {},
   "source": [
    "**Reading a shapefile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648b67d-55cc-4b1b-89d0-416701d2df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp = gp.read_file('../../data/NIWO_ShrubDensity.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7c1d8-d55d-44d3-a005-0bf48c7a2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_plot = shp.hvplot(tiles='ESRI', color='blue', alpha=0.5, crs='EPSG:4326')\n",
    "geojson_plot * shp_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfd49e-9cda-4626-8a0f-1b36517c3fd4",
   "metadata": {},
   "source": [
    "We can get the bounding box that encompasses all of the features using the `total_bounds` method and pass it to the `bounding_box` parameter in our search query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38921a-0a0c-4c34-8f0a-2cb18f95935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = tuple(list(shp.total_bounds))\n",
    "bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eaf940-33db-49d4-b19f-85822c122a79",
   "metadata": {},
   "source": [
    "#### Specify Temporal Parameters  \n",
    "\n",
    "To specify the dates we are interested in, we pass a tuple containing the start and end date in the form of yyyy-mm-dd.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42def03f-cc00-4c0f-958e-93b65b872916",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = ('2023-05-01','2023-09-30')    # tuple containing the start and end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b2a52-0fdc-4b4b-a4ff-6ffafe2e1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "granules_request = earthaccess.search_data(\n",
    "    short_name='ECO_L2T_LSTE',\n",
    "    version='002',\n",
    "    provider='LPCLOUD',\n",
    "    bounding_box=bbox,\n",
    "    temporal=date,\n",
    "    count=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d4bab7",
   "metadata": {},
   "source": [
    "If we wanted to perform a query for a point location, we would only need to swap out the **bounding_bbox** parameter for the **point** parameter. The input for the **point** parameter is a tuple containing a longitude and latitude coordinate pair. For example:\n",
    "\n",
    "```\n",
    "point=(-105.58650854045227,40.05184049311418)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1a1d8-468f-4530-808f-f0b490eb98f5",
   "metadata": {},
   "source": [
    "Let's look at the first granule from our bounding box query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f65d8-a9d1-461a-ac34-bf4f31b8c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule = granules_request[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e7a05-3134-45d1-ae1f-cadc89f92de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99196be-d618-49fe-a8cd-7e9a90a1d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule['umm'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0fee2-ee65-4a4d-a443-82bf1c4f2b27",
   "metadata": {},
   "source": [
    "The `DataGranule` class also has several convenience methods. The `data_links()` method can extract all of the data links associated with each granule.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d714974-de98-40e0-a0dc-3aa2888b7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule.data_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c971cb-9936-4d4f-904f-b30cffca2dff",
   "metadata": {},
   "source": [
    "Granules for **ECO_L2T_LSTE** are made up of multiple files. This is the case for several of the `LPCLOUD` provider collection. Other collections may only have a single file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa8035-f4a8-4592-b19c-49d5c06331fb",
   "metadata": {},
   "source": [
    "#### Printing data granules  \n",
    "\n",
    "Since we are in a notebook we can take advantage of it to see a more user friendly version of the granules with the built-in function `display`. This will render browse image for the granule if available and eventually will have a similar representation as the one from the Earthdata Search client.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd5f5c-a854-4a72-a831-33b8bd7ce9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing 2 granules using display\n",
    "[display(granule) for granule in granules_request[0:2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c90c43-6e17-42f5-8bf5-95fdd3cb0dce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b247449d-448b-41c5-b52d-a5b4e4dff079",
   "metadata": {},
   "source": [
    "## Working with `earthaccess` Data Links  \n",
    "\n",
    "With `earthaccess` we can get the files regardless if they are on-prem or cloud based with the same API call, although an important consideration is that if we want to access data in the cloud (direct access) we must run the code in the cloud. This is because some S3 buckets are configured to only allow direct access (s3:// links) if the requester is in the same zone, `us-west-2`.  \n",
    "\n",
    "### Streaming Data - Reading a Cloud Optimized GeoTIFF (COG) File  \n",
    "\n",
    "Currently, `earthaccess` doesn't have any helper methods to aid in reading COG files. Fortunately, the `rioxarray` library has great support for reading COG files. NASA, however, requires authentication when accessing NASA data. Below, we create a runtime context (**rio_env**) that will pass along our Earthdata Login credentials from a **.netrc** file stored in our home directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be624221-013d-43c0-8679-ff945be552d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import hvplot\n",
    "import hvplot.xarray\n",
    "\n",
    "rio_env = rio.Env(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR',\n",
    "                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\n",
    "rio_env.__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2e0ce6",
   "metadata": {},
   "source": [
    "ECO_L2T_LSTE version 2 contain multiple files per granules. In this case, multiple cloud optimized GeoTIFF (COG) files. Here we are only interested in the **LST** (land surface temperature) files. In an analysis we'd need to consult the other associated files for quality control. We'll use a Python list comprehension to return only data files that contain **LST.tif** in their file name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb98b6d-072c-4c48-a773-a12784690520",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_links = [l for dl in granules_request for l in dl.data_links() if 'LST.tif' in l]\n",
    "lst_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d82499",
   "metadata": {},
   "source": [
    "Now we'll use `rioxarray` to read the first file in our list. `rioxarray` reads the COG file in as an `xarray` `DataArray`. Normally when a COG file is read in using `rioxarray`, a **band** coordinate variable is created. In most circumstances this coordinate variable is not need. We use the `squeeze` function to remove **band** from our object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3b98c-1465-46b8-9147-632e7ccea52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_da = rxr.open_rasterio(filename=lst_links[0]).squeeze('band', drop=True)\n",
    "lst_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af654e-5e92-4938-9b1f-bed71427828c",
   "metadata": {},
   "source": [
    "We can do a cursory plot of the data using `hvplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f6075-c149-424a-9889-d6a84e4a63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_opts = dict(frame_height=405, frame_width=720, fontscale=2)\n",
    "\n",
    "lst_da.rio.reproject('EPSG:4326').hvplot.image(x='x', y='y', **size_opts, cmap='inferno', tiles='ESRI', crs='EPSG:4326') * shp.hvplot(tiles='ESRI', color = '#FF000000', crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39108967-2da8-4535-b212-9d12acdbd016",
   "metadata": {},
   "source": [
    "### Download `earthaccess` data links\n",
    "\n",
    "`earthaccess` provides a helper function for downloading `earthaccess` `data_links` objects or a list of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b11367-7f41-40fa-b28a-963b61300af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.download(lst_links[0], local_path='/home/jovyan/HYR-SENSE/data/output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
